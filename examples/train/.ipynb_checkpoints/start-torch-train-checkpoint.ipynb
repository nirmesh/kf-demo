{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cced1ffd-de1f-464f-81ba-168914fed59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp.dsl import component, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d87e37cc-83d7-4724-b806-72059fb5bfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image='python:3.9',\n",
    "    packages_to_install=[\"git+https://github.com/kubeflow/trainer.git@master#subdirectory=sdk\"],\n",
    ")\n",
    "def start_training_job() -> str:\n",
    "    from kubeflow.trainer import CustomTrainer, TrainerClient\n",
    "\n",
    "\n",
    "    def train_pytorch():\n",
    "        import os\n",
    "    \n",
    "        import torch\n",
    "        from torch import nn\n",
    "        import torch.nn.functional as F\n",
    "    \n",
    "        from torchvision import datasets, transforms\n",
    "        import torch.distributed as dist\n",
    "        from torch.utils.data import DataLoader, DistributedSampler\n",
    "    \n",
    "        # [1] Configure CPU/GPU device and distributed backend.\n",
    "        # Kubeflow Trainer will automatically configure the distributed environment.\n",
    "        device, backend = (\"cuda\", \"nccl\") if torch.cuda.is_available() else (\"cpu\", \"gloo\")\n",
    "        dist.init_process_group(backend=backend)\n",
    "    \n",
    "        local_rank = int(os.getenv(\"LOCAL_RANK\", 0))\n",
    "        print(\n",
    "            \"Distributed Training with WORLD_SIZE: {}, RANK: {}, LOCAL_RANK: {}.\".format(\n",
    "                dist.get_world_size(),\n",
    "                dist.get_rank(),\n",
    "                local_rank,\n",
    "            )\n",
    "        )\n",
    "    \n",
    "        # [2] Define PyTorch CNN Model to be trained.\n",
    "        class Net(nn.Module):\n",
    "            def __init__(self):\n",
    "                super(Net, self).__init__()\n",
    "                self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "                self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "                self.fc1 = nn.Linear(4 * 4 * 50, 500)\n",
    "                self.fc2 = nn.Linear(500, 10)\n",
    "    \n",
    "            def forward(self, x):\n",
    "                x = F.relu(self.conv1(x))\n",
    "                x = F.max_pool2d(x, 2, 2)\n",
    "                x = F.relu(self.conv2(x))\n",
    "                x = F.max_pool2d(x, 2, 2)\n",
    "                x = x.view(-1, 4 * 4 * 50)\n",
    "                x = F.relu(self.fc1(x))\n",
    "                x = self.fc2(x)\n",
    "                return F.log_softmax(x, dim=1)\n",
    "    \n",
    "        # [3] Attach model to the correct device.\n",
    "        device = torch.device(f\"{device}:{local_rank}\")\n",
    "        model = nn.parallel.DistributedDataParallel(Net().to(device))\n",
    "        model.train()\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "    \n",
    "        # [4] Get the Fashion-MNIST dataset and distributed it across all available devices.\n",
    "        dataset = datasets.FashionMNIST(\n",
    "            \"./data\",\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=transforms.Compose([transforms.ToTensor()]),\n",
    "        )\n",
    "        train_loader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=100,\n",
    "            sampler=DistributedSampler(dataset),\n",
    "        )\n",
    "    \n",
    "        # [5] Define the training loop.\n",
    "        for epoch in range(3):\n",
    "            for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "                # Attach tensors to the device.\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "    \n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                loss = F.nll_loss(outputs, labels)\n",
    "    \n",
    "                # Backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                if batch_idx % 10 == 0 and dist.get_rank() == 0:\n",
    "                    print(\n",
    "                        \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                            epoch,\n",
    "                            batch_idx * len(inputs),\n",
    "                            len(train_loader.dataset),\n",
    "                            100.0 * batch_idx / len(train_loader),\n",
    "                            loss.item(),\n",
    "                        )\n",
    "                    )\n",
    "    \n",
    "        # Wait for the training to complete and destroy to PyTorch distributed process group.\n",
    "        dist.barrier()\n",
    "        if dist.get_rank() == 0:\n",
    "            print(\"Training is finished\")\n",
    "        dist.destroy_process_group()\n",
    "    \n",
    "    \n",
    "    job_id = TrainerClient().train(\n",
    "        trainer=CustomTrainer(\n",
    "            func=train_pytorch,\n",
    "            num_nodes=1,\n",
    "            resources_per_node={\n",
    "                \"cpu\": 5,\n",
    "                \"memory\": \"16Gi\",\n",
    "                \"gpu\": 1, # Comment this line if you don't have GPUs.\n",
    "            },\n",
    "        ),\n",
    "        runtime=TrainerClient().get_runtime(\"torch-distributed\"),\n",
    "    )\n",
    "    \n",
    "    print(job_id)\n",
    "    \n",
    "    for s in TrainerClient().get_job(name=job_id).steps:\n",
    "        print(f\"Step: {s.name}, Status: {s.status}, Devices: {s.device} x {s.device_count}\")\n",
    "\n",
    "    return job_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b02c792f-6b04-44bd-b3a8-d90dd9713b43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/eab56b7f-f1ae-4d56-a13b-ae35c83302b6\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/4f38aaf7-281f-4fa1-978e-8a2292dfe9c5\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@pipeline(name='start-training-job-pipeline')\n",
    "def start_training_job_pipeline():\n",
    "    step1 = start_training_job()\n",
    "    print(step1.output)\n",
    "\n",
    "client = kfp.Client()\n",
    "kfp.compiler.Compiler().compile(start_training_job_pipeline, 'start_training_job_pipeline.yaml')\n",
    "run = client.create_run_from_pipeline_func(start_training_job_pipeline, arguments={}, enable_caching=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe50fd53-490d-4b2b-b3ad-910546de4f79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
